# 从内核角度看 IO 模型

## 前言

我们来看下Netty 官网首页的简介，

![img](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/1730004384512-218a7b37-ac5a-446c-8eec-d9cab6a440aa.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

翻译后为：

- Netty 是一个异步事件驱动的网络应用框架，旨在快速开发可维护的高性能协议服务器和客户端。  
- Netty 是一个 NIO 客户端-服务器框架，能够快速且轻松地开发网络应用程序，如协议服务器和客户端。它极大地简化和优化了网络编程，例如 TCP 和 UDP 套接字服务器。
- “快速和轻松”并不意味着最终应用会遭受可维护性或性能问题。Netty 在设计时充分借鉴了实现众多协议（如 FTP、SMTP、HTTP 以及各种基于二进制和文本的传统协议）的经验。因此，Netty 成功地找到了在开发便捷性、性能、稳定性和灵活性之间实现平衡的方法，没有任何妥协。

我们可以推测出 Netty 这个框架的高性能有很大部分源于其底层模型：

* IO 模型：NIO，当然你也可以用其他的，但是主要还是NIO
* IO 线程模型：Reactor 模型

所以本文和 [IO 多路复用的操作系统支持](https://www.yuque.com/onejava/gwzrgm/mrhc4wo4wqbd4csq) 旨在说明这两部分的内容

## 网络包接收流程

![image-20241029174752265](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029174752265.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

### 性能开销

- 应用程序通过 系统调用 从 用户态 转为 内核态 的开销，以及系统调用返回时从内核态转为用户态的开销
- 网络数据从内核空间通过CPU拷贝到用户空间的开销
- 内核线程  `ksoftirqd` 响应软中断的开销
- CPU 响应硬中断的开销
- DMA拷贝网络数据包到内存中的开销

- 

## 再谈【阻塞&非阻塞】与【同步&异步】

经过前边对网络数据包接收流程的介绍，在这里我们可以将整个流程总结为两个阶段：

![image-20241029181409123](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029181409123.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)



- **数据准备阶段：** 在这个阶段，网络数据包到达网卡，通过`DMA`的方式将数据包拷贝到内存中，然后经过硬中断，软中断，接着通过内核线程`ksoftirqd`经过内核协议栈的处理，最终将数据发送到`内核Socket`的接收缓冲区中。
- **数据拷贝阶段：** 当数据到达`内核Socket`的接收缓冲区中时，此时数据存在于`内核空间`中，需要将数据`拷贝`到`用户空间`中，才能够被应用程序读取。

### 阻塞&非阻塞

阻塞与非阻塞的区别主要发生在第一阶段：`数据准备阶段`。

当应用程序发起`系统调用read`时，线程从用户态转为内核态，读取内核`Socket`的接收缓冲区中的网络数据。

#### 阻塞

如果这时内核`Socket`的接收缓冲区没有数据，那么线程就会一直`等待`，直到`Socket`接收缓冲区有数据为止。随后将数据从内核空间拷贝到用户空间，`系统调用read`返回。

![image-20241029182536235](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029182536235.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

从图中我们可以看出：**阻塞**的特点是在第一阶段和第二阶段`都会等待`。

#### 非阻塞

`阻塞`和`非阻塞`主要的区分是在第一阶段：`数据准备阶段`。

- 在第一阶段，当`Socket`的接收缓冲区中没有数据的时候，`阻塞模式下`应用线程会一直等待。`非阻塞模式下`应用线程不会等待，`系统调用`直接返回错误标志`EWOULDBLOCK`。
- 当`Socket`的接收缓冲区中有数据的时候，`阻塞`和`非阻塞`的表现是一样的，都会进入第二阶段`等待`数据从`内核空间`拷贝到`用户空间`，然后`系统调用返回`。

![image-20241029182840695](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029182840695.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

从上图中，我们可以看出：**非阻塞**的特点是第一阶段`不会等待`，但是在第二阶段还是会`等待`。

### 同步&异步

`同步`与`异步`主要的区别发生在第二阶段：`数据拷贝阶段`。

前边我们提到在`数据拷贝阶段`主要是将数据从`内核空间`拷贝到`用户空间`。然后应用程序才可以读取数据。

当内核`Socket`的接收缓冲区有数据到达时，进入第二阶段。

#### 同步

`同步模式`在数据准备好后，是由`用户线程`的`内核态`来执行`第二阶段`。所以应用程序会在第二阶段发生`阻塞`，直到数据从`内核空间`拷贝到`用户空间`，系统调用才会返回。

Linux下的 `epoll`和Mac 下的 `kqueue`都属于`同步 IO`。

![image-20241029183327859](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029183327859.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

#### 异步

`异步模式`下是由`内核`来执行第二阶段的数据拷贝操作，当`内核`执行完第二阶段，会通知用户线程IO操作已经完成，并将数据回调给用户线程。所以在`异步模式`下 `数据准备阶段`和`数据拷贝阶段`均是由`内核`来完成，不会对应用程序造成任何阻塞。

基于以上特征，我们可以看到`异步模式`需要内核的支持，比较依赖操作系统底层的支持。

在目前流行的操作系统中，只有Windows 中的 `IOCP`才真正属于异步 IO，实现的也非常成熟。但Windows很少用来作为服务器使用。

而常用来作为服务器使用的Linux，`异步IO机制`实现的不够成熟，与NIO相比性能提升的也不够明显。

但Linux kernel 在5.1版本由Facebook的大神Jens Axboe引入了新的异步IO库`io_uring` 改善了原来Linux native AIO的一些性能问题。性能相比`Epoll`以及之前原生的`AIO`提高了不少，值得关注。

![image-20241029192907117](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029192907117.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

## IO 模型

在进行网络IO操作时，用什么样的IO模型来读写数据将在很大程度上决定了网络框架的IO性能。所以IO模型的选择是构建一个高性能网络框架的基础。

在《UNIX 网络编程》一书中介绍了五种IO模型：`阻塞IO`,`非阻塞IO`,`IO多路复用`,`信号驱动IO`,`异步IO`，每一种IO模型的出现都是对前一种的升级优化。

### 阻塞IO

![image-20241029183926317](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029183926317.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

#### 读

当用户线程发起`read`系统调用，用户线程从用户态切换到内核态，在内核中去查看`Socket`接收缓冲区是否有数据到来。

- `Socket`接收缓冲区中`有数据`，则用户线程在内核态将内核空间中的数据拷贝到用户空间，系统IO调用返回。
- `Socket`接收缓冲区中`无数据`，则用户线程让出CPU，进入`阻塞状态`。当数据到达`Socket`接收缓冲区后，内核唤醒`阻塞状态`中的用户线程进入`就绪状态`，随后经过CPU的调度获取到`CPU quota`进入`运行状态`，将内核空间的数据拷贝到用户空间，随后系统调用返回。

#### 写

当用户线程发起`send`系统调用时，用户线程从用户态切换到内核态，将发送数据从用户空间拷贝到内核空间中的`Socket`发送缓冲区中。

- 当`Socket`发送缓冲区能够容纳下发送数据时，用户线程会将全部的发送数据写入`Socket`缓冲区，然后执行在《网络包发送流程》这小节介绍的后续流程，然后返回。
- 当`Socket`发送缓冲区空间不够，无法容纳下全部发送数据时，用户线程让出CPU,进入`阻塞状态`，直到`Socket`发送缓冲区能够容纳下全部发送数据时，内核唤醒用户线程，执行后续发送流程。

`阻塞IO`模型下的写操作做事风格比较硬刚，非得要把全部的发送数据写入发送缓冲区才肯善罢甘休。

#### 对应 IO 线程模型

![image-20241029184810320](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029184810320.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

由于`阻塞IO`的读写特点，所以导致在`阻塞IO`模型下，每个请求都需要被一个独立的线程处理。一个线程在同一时刻只能与一个连接绑定。来一个请求，服务端就需要创建一个线程用来处理请求。

当客户端请求的并发量突然增大时，服务端在一瞬间就会创建出大量的线程，而创建线程是需要系统资源开销的，这样一来就会一瞬间占用大量的系统资源。

如果客户端创建好连接后，但是一直不发数据，通常大部分情况下，网络连接也`并不`总是有数据可读，那么在空闲的这段时间内，服务端线程就会一直处于`阻塞状态`，无法干其他的事情。CPU也`无法得到充分的发挥`，同时还会`导致大量线程切换的开销`。

#### 适用场景

基于以上`阻塞IO模型`的特点，该模型只适用于`连接数少`，`并发度低`的业务场景。

比如公司内部的一些管理系统，通常请求数在100个左右，使用`阻塞IO模型`还是非常适合的。而且性能还不输NIO。

该模型在C10K之前，是普遍被采用的一种IO模型。

### 非阻塞 IO

`阻塞IO模型`最大的问题就是一个线程只能处理一个连接，如果这个连接上没有数据的话，那么这个线程就只能阻塞在系统IO调用上，不能干其他的事情

![image-20241029182840695](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029182840695.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

#### 读

当用户线程发起非阻塞`read`系统调用时，用户线程从`用户态`转为`内核态`，在内核中去查看`Socket`接收缓冲区是否有数据到来。

- `Socket`接收缓冲区中`无数据`，系统调用立马返回，并带有一个 `EWOULDBLOCK` 或 `EAGAIN`错误，这个阶段用户线程`不会阻塞`，也`不会让出CPU`，而是会继续`轮训`直到`Socket`接收缓冲区中有数据为止。
- `Socket`接收缓冲区中`有数据`，用户线程在`内核态`会将`内核空间`中的数据拷贝到`用户空间`，**注意**这个数据拷贝阶段，应用程序是`阻塞的`，当数据拷贝完成，系统调用返回。

#### 写

前边我们在介绍`阻塞写`的时候提到`阻塞写`的风格特别的硬朗，头比较铁非要把全部发送数据一次性都写到`Socket`的发送缓冲区中才返回，如果发送缓冲区中没有足够的空间容纳，那么就一直阻塞死等，特别的刚。

相比较而言`非阻塞写`的特点就比较佛系，当发送缓冲区中没有足够的空间容纳全部发送数据时，`非阻塞写`的特点是`能写多少写多少`，写不下了，就立即返回。并将写入到发送缓冲区的字节数返回给应用程序，方便用户线程不断的`轮训`尝试将`剩下的数据`写入发送缓冲区中。

#### 模型

![image-20241029193518417](https://echo798.oss-cn-shenzhen.aliyuncs.com/img/image-20241029193518417.png?x-oss-process=image/watermark,image_aW1nL3dhdGVyLnBuZw==,g_nw,x_1,y_1)

基于以上`非阻塞IO`的特点，我们就不必像`阻塞IO`那样为每个请求分配一个线程去处理连接上的读写了。

我们可以利用**一个线程或者很少的线程**，去`不断地轮询`每个`Socket`的接收缓冲区是否有数据到达，如果没有数据，`不必阻塞`线程，而是接着去`轮询`下一个`Socket`接收缓冲区，直到轮询到数据后，处理连接上的读写，或者交给业务线程池去处理，轮询线程则`继续轮询`其他的`Socket`接收缓冲区。

这样一个`非阻塞IO模型`就实现了我们在本小节开始提出的需求：**我们需要用尽可能少的线程去处理更多的连接**

#### 适用场景

虽然`非阻塞IO模型`与`阻塞IO模型`相比，减少了很大一部分的资源消耗和系统开销。

但是它仍然有很大的性能问题，因为在`非阻塞IO模型`下，需要用户线程去`不断地`发起`系统调用`去轮训`Socket`接收缓冲区，这就需要用户线程不断地从`用户态`切换到`内核态`，`内核态`切换到`用户态`。随着并发量的增大，这个上下文切换的开销也是巨大的。

所以单纯的`非阻塞IO`模型还是无法适用于高并发的场景。只能适用于`C10K`以下的场景。

### IO多路复用

请移步 [IO 多路复用基础](https://www.yuque.com/onejava/gwzrgm/mrhc4wo4wqbd4csq)



### 信号驱动IO

### 异步IO
